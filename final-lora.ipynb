{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":120000,"sourceType":"modelInstanceVersion","modelInstanceId":100931,"modelId":121027}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Create a kaggle account https://www.kaggle.com/\n# 2. create a wnb account https://wandb.ai/site/\n# 3. model load steps is defined below while implemetation","metadata":{}},{"cell_type":"markdown","source":"# install libraries\n- transformer - huggingface lib provide the pretraind transformer based model\n- accelerate - hugging face lib allows for quickly set up models for training on different hardware configurations  without needing to change the code.\n- peft -fine-tuning process for large models, helping you fine-tune them with fewer parameters.\n- dataset - huggingface lib for the loading and managing the dataset","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers accelerate peft datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:34:00.392639Z","iopub.execute_input":"2024-11-14T16:34:00.393521Z","iopub.status.idle":"2024-11-14T16:34:13.287352Z","shell.execute_reply.started":"2024-11-14T16:34:00.393480Z","shell.execute_reply":"2024-11-14T16:34:13.286012Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"import torch # provides tensors and neural network functionality.\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n# 1. huggingface lib tool to tokenize the text using the pretrained modelonvert raw text into tokenized input that a model can process. It handles tasks like splitting text into words, converting them to IDs\n# 2. load the pretrain language model for text generation \n# 3. create a mask language modeling data make sure each batch has uniform length and structure.\nfrom datasets import load_dataset # load dataset\nfrom peft import get_peft_model, LoraConfig # 1. combine the model and lora config  \nfrom transformers import Trainer, TrainingArguments # 1.train and eval the model in nlp task 2. class to set train config like LR, batch size \nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:34:17.268507Z","iopub.execute_input":"2024-11-14T16:34:17.268915Z","iopub.status.idle":"2024-11-14T16:34:37.586037Z","shell.execute_reply.started":"2024-11-14T16:34:17.268875Z","shell.execute_reply":"2024-11-14T16:34:37.585220Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# * Load the Model and Tokenizer*\r\n\r\n***Define the path for the pretrained LLaMA model & tokenizer.***\r\n\r\n***We are using Llama 3.2-1B for summarization.***\r\n\r\n***You can request the model here - https://www.llama.com/llama-downloads/***\r\n- Go to the https://www.llama.com/llama-downloads/ website and fill out the form with your information. Select both lightweight and vision models. \r\n <img src = \"https://lh7-rt.googleusercontent.com/docsz/AD_4nXcFiUrvxq64Fhs_6llqddOvsWsVdoqCDVXHQ6aD3qJdGa18x--DbqkchRWZKTDxHPi_Q7iKgo70uUaVyVtWf3qc5N4PIganyeOU0Iok9seaZ6-zhQNjI5WZSIfTra4GzmeYij4OOmMWpT8J_apNwxM65lTr?key=BktxNAbDzZ2rY2knU23WEA\" height=750 width=750>\r\n\r\n - Go to the Meta | Llama 3.2 model page o kaggle and click the “Submit Form” button. \r\n <img src = \"https://lh7-rt.googleusercontent.com/docsz/AD_4nXfH27KjswHaCkhkYV_riCqMeU7uyZyXiJlZBZRgPTn9kjeTk4YEBnHvdCD5U5ekS6X7Jpq8El8nCWT5qJfop5xz3jLU_u2zdyi89nss0VrMWXrUgLryyyGSij5qivA9q0GwIuSKm-mGQK3C-4pxxubEBbZh?key=BktxNAbDzZ2rY2knU23WEA\" height=750 width=750>\r\n\r\n - Wait a few minutes until you see the option to either download or create the new notebook. Select the Transformers tab and model variation, then click the “+ New Notebook” button.\r\n <img src = \"https://lh7-rt.googleusercontent.com/docsz/AD_4nXe-i6xbHHcVR3CnH009oxWJfDDLUKrdBQnINzaR3342u0KRrOmtz5RCsQXg0q_uWA62OqWHTJKS2jQRDmVcWgVrdh_6OixTUhEf-mnYXFQ7AVRHnPC_VNFlLVHeWY0T2EkvL0lSp52ssERvM-2FoIIHpM9Y?key=BktxNAbDzZ2rY2knU23WEA\" height=750 width=750>\r\n\r\n - click on new notebook with transformer option \r\n\r\n***Once you have access to model add it to kaggle input***","metadata":{}},{"cell_type":"code","source":"# Define the model path\nmodel_path = \"/kaggle/input/llama-3.2/transformers/1b/1\"\n#Loads a pre-trained tokenizer from the specified model path.\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# Set device (CUDA if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the model and move it to the device\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    trust_remote_code=True, # allows for the including the custom configuration\n    torch_dtype=torch.float16,   # This specifies the data type for the model's weights and computations.torch.float16 refers to half-precision floating point \n).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:34:40.589168Z","iopub.execute_input":"2024-11-14T16:34:40.589884Z","iopub.status.idle":"2024-11-14T16:34:55.354395Z","shell.execute_reply.started":"2024-11-14T16:34:40.589845Z","shell.execute_reply":"2024-11-14T16:34:55.353556Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# setting up lora configuraton\n-<img src = \"https://miro.medium.com/v2/resize:fit:720/format:webp/1*rOW5plKBuMlGgpD0SO8nZA.png\" height=750 width=750> \\n\n\n- The pre-trained parameters of the original model (W) are frozen. During training, these weights will not be modified.\n- A new set of parameters is concurrently added to the networks WA and WB. These networks utilize low-rank weight vectors, where the dimensions of these vectors are represented as dxr and rxd. Here, ‘d’ stands for the dimension of the original frozen network parameters vector, while ‘r’ signifies the chosen low-rank or lower dimension\n- value of r should be smaller to simplified model training process\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Set up LoRA config for PEFT\nlora_config = LoraConfig(\n    r=8, # rank of low-rank metrix used\n    lora_alpha=32, # scale factor that controls how much the low-rank updates affect the model.\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n#     q_proj: Query projection.\n#     k_proj: Key projection.\n#     v_proj: Value projection.\n#      o_proj: Output projection\n    lora_dropout=0.1, # 0.1 means that 10% of the low-rank parameters will be dropped during training.\n    bias=\"none\" # no bias term\n)\n\n# Apply PEFT to the modelx\nmodel = get_peft_model(model, lora_config)\n\n# Freeze parameters that are not LoRA\nfor name, param in model.named_parameters():\n    if \"lora\" not in name:  # Freeze parameters that are not LoRA\n        param.requires_grad = False\n    else:\n        param.requires_grad = True  # LoRA parameters should be trainable","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:34:57.522554Z","iopub.execute_input":"2024-11-14T16:34:57.523414Z","iopub.status.idle":"2024-11-14T16:34:57.687103Z","shell.execute_reply.started":"2024-11-14T16:34:57.523370Z","shell.execute_reply":"2024-11-14T16:34:57.686177Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Dataset loading\n- srource :  https://huggingface.co/datasets/EdinburghNLP/xsum\n- Extreme Summarization (XSum) Dataset.\r\n- \r\nThere are three featur\n- \r\n\r\ndocument: Input news artic- le.\r\nsummary: One sentence summary of the arti- cle.\r\nid: BBC ID of the ar\n- news artical summery dataset\n- we are using only 5% dataset for faster training ticle.","metadata":{}},{"cell_type":"code","source":"# Load the dataset (subsetting for testing)\n# function from the datasets library that fetches various NLP datasets\ndataset = load_dataset(\"xsum\", split=\"train[:5%]\")\n# Set the pad_token to be the same as eos_token\n# is pad token is not set than set that using the eos_token\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n#Converts text data into tokenized numerical data that can be processed by the model\n# Tokenization function\ndef tokenize_function(examples):\n    return tokenizer(examples[\"document\"], padding=\"max_length\", truncation=True, max_length=512)\n\n# Apply the tokenization function to the dataset and it process batch wise\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\n\n# Remove unnecessary columns\n# because we have get our tokens\ntokenized_dataset = tokenized_dataset.remove_columns([\"document\", \"summary\", \"id\"])\n\n# Check the columns in the tokenized dataset\nprint(tokenized_dataset.column_names)\n\n# Split the dataset into training and validation (using same for both in this case)\ntrain_dataset = tokenized_dataset\neval_dataset = tokenized_dataset  # For this small example, using the same dataset for eval\n\n# Prepare data collator\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:35:02.084917Z","iopub.execute_input":"2024-11-14T16:35:02.085825Z","iopub.status.idle":"2024-11-14T16:37:06.829819Z","shell.execute_reply.started":"2024-11-14T16:35:02.085760Z","shell.execute_reply":"2024-11-14T16:37:06.828832Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"xsum.py:   0%|          | 0.00/5.76k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eecdc3463914b3da49388d935069c00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/6.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b57c866c407a44c7b68e1be6d7b99df1"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for xsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/xsum.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"(…)SUM-EMNLP18-Summary-Data-Original.tar.gz:   0%|          | 0.00/255M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"899efe030fc44ad5b6acb5d39d9745e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.72M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abd9631d546e4051aa46a5f3c4506266"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/204045 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc25d7c025f842a9b815ae3466cf041d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/11332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7083dd2102d44b18ba45ade7a23f9c3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11334 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d949c734ba41495993b4983255256662"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10202 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce262e04f8f746a2877dfa9b340bd214"}},"metadata":{}},{"name":"stdout","text":"['input_ids', 'attention_mask']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# setting up the training configuration ","metadata":{}},{"cell_type":"code","source":"# Setup training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\", #  Directory to save training results, checkpoints\n    evaluation_strategy=\"epoch\", # run code epoch wise \"epoch\" means evaluation will occur at the end of each training epoch.\n    per_device_train_batch_size=2, # batch size for each training step per device\n    per_device_eval_batch_size=2, # for eval\n    num_train_epochs=1, # num of epoch]\n    save_strategy=\"epoch\", # \n    logging_dir=\"./logs\", # Directory where log files will be stored.\n    logging_steps=10, # model logs training metrics every 10 steps.\n    learning_rate=5e-5, # LR\n    remove_unused_columns=False, # Keeps all columns in the dataset, which may be useful if the model uses multiple input features.\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model, # model\n    args=training_args, # training config\n    train_dataset=train_dataset, # train dataset\n    eval_dataset=eval_dataset, #eval dataset\n    data_collator=data_collator,# data collator to mask text padding..\n)\n\n# Function to generate a response from the model\ndef generate_response(model, tokenizer, prompt, device):\n    # convert the promt into the token\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        output = model.generate(**inputs, max_new_tokens=100)  # Limit the number of tokens generated\n    response = tokenizer.decode(output[0], skip_special_tokens=True) # special_token = omitting special tokens like <EOS> or <PAD> with skip_special_tokens=True.\n    return response\n\n# Sample text for inference\nsample_text = \"My name is harshil i am \"\n\n# Get model response before training\nprint(\"Generating response before training...\")\nresponse_before = generate_response(model, tokenizer, sample_text, device)\nprint(f\"Response before training:\\n{response_before}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:37:11.537567Z","iopub.execute_input":"2024-11-14T16:37:11.538512Z","iopub.status.idle":"2024-11-14T16:37:16.579101Z","shell.execute_reply.started":"2024-11-14T16:37:11.538464Z","shell.execute_reply":"2024-11-14T16:37:16.578121Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generating response before training...\n","output_type":"stream"},{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"Response before training:\nMy name is harshil i am 18 years old and i am from india. i am a student of 12th class and i am very interested in computer science. i am very good in programming and i am very good in maths. i am very good in english and i am very good in hindi. i am very good in science and i am very good in social science. i am very good in sports and i am very good in games. i am very good in studies and i am very good in sports. i am\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# 1. goto https://wandb.ai/\n# 2. sign up in website\n# 3. go to  https://wandb.ai/authorize\n# 4. copy the authorization key \n# 5. paste the key and press enter","metadata":{}},{"cell_type":"code","source":"# Train the model\ntrainer.train() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:37:29.612683Z","iopub.execute_input":"2024-11-14T16:37:29.613102Z","iopub.status.idle":"2024-11-14T18:32:44.477351Z","shell.execute_reply.started":"2024-11-14T16:37:29.613061Z","shell.execute_reply":"2024-11-14T18:32:44.476536Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112862588888422, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3685e943469a4af8a18f383a43e6ddee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241114_163803-wqcm7fhb</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/dummyacct165-nirma-university/huggingface/runs/wqcm7fhb' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/dummyacct165-nirma-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/dummyacct165-nirma-university/huggingface' target=\"_blank\">https://wandb.ai/dummyacct165-nirma-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/dummyacct165-nirma-university/huggingface/runs/wqcm7fhb' target=\"_blank\">https://wandb.ai/dummyacct165-nirma-university/huggingface/runs/wqcm7fhb</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5101' max='5101' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5101/5101 1:54:36, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.391200</td>\n      <td>No log</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5101, training_loss=2.399479469255568, metrics={'train_runtime': 6913.3202, 'train_samples_per_second': 1.476, 'train_steps_per_second': 0.738, 'total_flos': 3.055233082274611e+16, 'train_loss': 2.399479469255568, 'epoch': 1.0})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Save the trained model\noutput_dir = \"/kaggle/working/FinalLora\"\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\nprint(f\"Model saved to {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T18:36:07.921061Z","iopub.execute_input":"2024-11-14T18:36:07.921481Z","iopub.status.idle":"2024-11-14T18:36:08.239079Z","shell.execute_reply.started":"2024-11-14T18:36:07.921441Z","shell.execute_reply":"2024-11-14T18:36:08.237834Z"}},"outputs":[{"name":"stdout","text":"Model saved to /kaggle/working/FinalLora\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import shutil\n\n# Path to the model directory\nmodel_dir = '/kaggle/working/FinalLora'\n\n# Path to save the zip file\nzip_path = '/kaggle/working/FinalLora.zip'\n\n# Zip the directory\nshutil.make_archive(zip_path.replace('.zip', ''), 'zip', model_dir)\n\nprint(f\"Model zipped at: {zip_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T18:37:09.271351Z","iopub.execute_input":"2024-11-14T18:37:09.271810Z","iopub.status.idle":"2024-11-14T18:37:10.143800Z","shell.execute_reply.started":"2024-11-14T18:37:09.271739Z","shell.execute_reply":"2024-11-14T18:37:10.142834Z"}},"outputs":[{"name":"stdout","text":"Model zipped at: /kaggle/working/FinalLora.zip\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\nfrom datasets import load_dataset # dataset loading\nimport torch\nimport numpy as np\nfrom sklearn.metrics import accuracy_score # to calculate accuracy\n\n# Load the saved model and tokenizer\nmodel_path = \"/kaggle/working/FinalLora\"  # Path to the saved model\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16)\n\n# Set device (CUDA if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Load the test dataset\ntest_dataset = load_dataset(\"xsum\", split=\"test[:5%]\")  # Using 5% for testing (adjust as needed)\n\n# Tokenization function\ndef tokenize_function(examples):\n    return tokenizer(examples[\"document\"], padding=\"max_length\", truncation=True, max_length=512)\n\n# Apply the tokenization function to the test dataset\ntokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# Remove unnecessary columns\ntokenized_test_dataset = tokenized_test_dataset.remove_columns([\"document\", \"summary\", \"id\"])\n\n# Prepare data collator\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n# Setup evaluation arguments (not training)\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_eval_batch_size=4,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    report_to=\"none\",  # Disable logging to other platforms (e.g., WandB, TensorBoard)\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    eval_dataset=tokenized_test_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\n# Evaluate the model on the test dataset\neval_results = trainer.evaluate()\n\n# Print the evaluation results\nprint(\"Evaluation Results:\", eval_results)\n\n#  Accuracy based on token-level comparison (this is an approximate metric for a language model)\ndef compute_accuracy(predictions, labels):\n    # Get the predicted tokens (argmax over vocab logits)\n    pred_ids = np.argmax(predictions, axis=-1)\n    # Flatten the token IDs to compare them\n    flat_pred_ids = pred_ids.flatten()\n    flat_labels = labels.flatten()\n    return np.sum(flat_pred_ids == flat_labels) / len(flat_labels)\n\n# Define a function to calculate accuracy per batch\ndef calculate_accuracy(batch):\n    inputs = tokenizer(batch[\"document\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n    labels = inputs.input_ids.clone()\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits = outputs.logits\n    predictions = logits.argmax(dim=-1)\n    return compute_accuracy(predictions.cpu().numpy(), labels.cpu().numpy())\n\n# Compute accuracy on the test dataset (for simplicity, we do a batch-wise calculation)\naccuracy = 0.0\nnum_batches = 0\n\nfor batch in test_dataset:\n    accuracy += calculate_accuracy(batch)\n    num_batches += 1\n\n# Compute the final accuracy\naccuracy = accuracy / num_batches\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T18:38:08.976650Z","iopub.execute_input":"2024-11-14T18:38:08.977375Z","iopub.status.idle":"2024-11-14T18:42:14.506746Z","shell.execute_reply.started":"2024-11-14T18:38:08.977333Z","shell.execute_reply":"2024-11-14T18:42:14.505693Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/567 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1f7c8bb5e3142a2a93676cf2d18f7e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='142' max='142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [142/142 01:54]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 2.3998351097106934, 'eval_model_preparation_time': 0.014, 'eval_runtime': 115.5709, 'eval_samples_per_second': 4.906, 'eval_steps_per_second': 1.229}\nTest Accuracy: 0.08%\n","output_type":"stream"}],"execution_count":12}]}