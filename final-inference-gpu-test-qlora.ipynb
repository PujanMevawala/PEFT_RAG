{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9908529,"sourceType":"datasetVersion","datasetId":6087833},{"sourceId":120000,"sourceType":"modelInstanceVersion","modelInstanceId":100931,"modelId":121027},{"sourceId":166980,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":142081,"modelId":164662}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# install libraries\n- transformer - huggingface lib provide the pretraind transformer based model\n- accelerate - hugging face lib allows for quickly set up models for training on different hardware configurations  without needing to change the code.\n- peft -fine-tuning process for large models, helping you fine-tune them with fewer parameters.\n- dataset - huggingface lib for the loading and managing the dataset\n- bitsandbytes - \nBitsAndBytesConfig is used in the Hugging Face transformers library to configure quantization and memory-efficient model loading,","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers accelerate peft datasets bitsandbytes\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-14T15:18:23.095128Z","iopub.execute_input":"2024-11-14T15:18:23.095471Z","iopub.status.idle":"2024-11-14T15:18:41.032770Z","shell.execute_reply.started":"2024-11-14T15:18:23.095435Z","shell.execute_reply":"2024-11-14T15:18:41.031577Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Importing the libraries","metadata":{}},{"cell_type":"code","source":"import torch  # provides tensors and neural network functionality.\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, BitsAndBytesConfig\n# 1. huggingface lib tool to tokenize the text using the pretrained modelonvert raw text into tokenized input that a model can process. It handles tasks like splitting text into words, converting them to IDs\n# 2. load the pretrain language model for text generation \n# 3. create a mask language modeling data make sure each batch has uniform length and structure.\n#4. load model on low precision\nfrom datasets import load_dataset # load dataset\nfrom peft import get_peft_model, LoraConfig # 1. combine the model and lora config  \nfrom transformers import Trainer, TrainingArguments # 1.train and eval the model in nlp task 2. class to set train config like LR, batch size \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T15:19:34.435076Z","iopub.execute_input":"2024-11-14T15:19:34.435479Z","iopub.status.idle":"2024-11-14T15:19:54.014227Z","shell.execute_reply.started":"2024-11-14T15:19:34.435438Z","shell.execute_reply":"2024-11-14T15:19:54.013282Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# * Load the Model and Tokenizer*\n\n***Define the path for the pretrained LLaMA model & tokenizer.***\n\n***We are using Llama 3.2-1B for summarization.***\n\n***You can request the model here - https://www.llama.com/llama-downloads/***\n- Go to the https://www.llama.com/llama-downloads/ website and fill out the form with your information. Select both lightweight and vision models. \n <img src = \"https://lh7-rt.googleusercontent.com/docsz/AD_4nXcFiUrvxq64Fhs_6llqddOvsWsVdoqCDVXHQ6aD3qJdGa18x--DbqkchRWZKTDxHPi_Q7iKgo70uUaVyVtWf3qc5N4PIganyeOU0Iok9seaZ6-zhQNjI5WZSIfTra4GzmeYij4OOmMWpT8J_apNwxM65lTr?key=BktxNAbDzZ2rY2knU23WEA\" height=750 width=750>\n\n - Go to the Meta | Llama 3.2 model page o kaggle and click the “Submit Form” button. \n <img src = \"https://lh7-rt.googleusercontent.com/docsz/AD_4nXfH27KjswHaCkhkYV_riCqMeU7uyZyXiJlZBZRgPTn9kjeTk4YEBnHvdCD5U5ekS6X7Jpq8El8nCWT5qJfop5xz3jLU_u2zdyi89nss0VrMWXrUgLryyyGSij5qivA9q0GwIuSKm-mGQK3C-4pxxubEBbZh?key=BktxNAbDzZ2rY2knU23WEA\" height=750 width=750>\n\n - Wait a few minutes until you see the option to either download or create the new notebook. Select the Transformers tab and model variation, then click the “+ New Notebook” button.\n <img src = \"https://lh7-rt.googleusercontent.com/docsz/AD_4nXe-i6xbHHcVR3CnH009oxWJfDDLUKrdBQnINzaR3342u0KRrOmtz5RCsQXg0q_uWA62OqWHTJKS2jQRDmVcWgVrdh_6OixTUhEf-mnYXFQ7AVRHnPC_VNFlLVHeWY0T2EkvL0lSp52ssERvM-2FoIIHpM9Y?key=BktxNAbDzZ2rY2knU23WEA\" height=750 width=750>\n\n - click on new notebook with transformer option \n\n***Once you have access to model add it to kaggle input***","metadata":{}},{"cell_type":"code","source":"# Define the model path\nmodel_path = \"/kaggle/input/llama-3.2/transformers/1b/1\"\n#Loads a pre-trained tokenizer from the specified model path.\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# Set device (CUDA if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Configure quantization with BitsAndBytesConfig for 4-bit or 8-bit precision\nquantization_config = BitsAndBytesConfig(\n    load_in_8bit=True,  # Set True for 8-bit quantization or False if using 4-bit\n)\n\n\n# Load the model with QLoRA and quantization configuration\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    trust_remote_code=True, # allows for the including the custom confihuration\n    quantization_config=quantization_config,  # Use the BitsAndBytesConfig for quantization\n    device_map=\"auto\",  # Automatically place model on available device(s)\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T15:20:44.115879Z","iopub.execute_input":"2024-11-14T15:20:44.116646Z","iopub.status.idle":"2024-11-14T15:20:57.020719Z","shell.execute_reply.started":"2024-11-14T15:20:44.116606Z","shell.execute_reply":"2024-11-14T15:20:57.019761Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# setting up lora configuraton\n-<img src = \"https://miro.medium.com/v2/resize:fit:720/format:webp/1*rOW5plKBuMlGgpD0SO8nZA.png\" height=750 width=750> \\n\n\n- The pre-trained parameters of the original model (W) are frozen. During training, these weights will not be modified.\n- A new set of parameters is concurrently added to the networks WA and WB. These networks utilize low-rank weight vectors, where the dimensions of these vectors are represented as dxr and rxd. Here, ‘d’ stands for the dimension of the original frozen network parameters vector, while ‘r’ signifies the chosen low-rank or lower dimension\n- value of r should be smaller to simplified model training process\n","metadata":{}},{"cell_type":"code","source":"# Set up LoRA config specifically for QLoRA\nlora_config = LoraConfig(\n    r=8,# rank of low-rank metrix used\n    lora_alpha=32,  # scale factor that controls how much the low-rank updates affect the model.\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    #     q_proj: Query projection.\n#     k_proj: Key projection.\n#     v_proj: Value projection.\n#      o_proj: Output projection\n    lora_dropout=0.1,  # 0.1 means that 10% of the low-rank parameters will be dropped during training.\n    bias=\"none\"  # no bias term\n)\n\n# Apply PEFT with QLoRA configuration to the model\nmodel = get_peft_model(model, lora_config)\n\n# Freeze parameters that are not part of LoRA\nfor name, param in model.named_parameters():\n    param.requires_grad = \"lora\" in name  # Only LoRA parameters should be trainable\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T15:21:02.531784Z","iopub.execute_input":"2024-11-14T15:21:02.532183Z","iopub.status.idle":"2024-11-14T15:21:02.666733Z","shell.execute_reply.started":"2024-11-14T15:21:02.532146Z","shell.execute_reply":"2024-11-14T15:21:02.665880Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Dataset loading\n- srource :  https://huggingface.co/datasets/EdinburghNLP/xsum\n- Extreme Summarization (XSum) Dataset.\n\n- There are three features\n- document: Input news article.\n- summary: One sentence summary of the article.\n- id: BBC ID of the article.\n- news artical summery dataset\n- we are using only 5% dataset for faster training ","metadata":{}},{"cell_type":"code","source":"# Load the dataset (subsetting for testing)\n# function from the datasets library that fetches various NLP datasets\ndataset = load_dataset(\"xsum\", split=\"train[:5%]\")\n\n# Set the pad_token to be the same as eos_token\n# is pad token is not set than set that using the eos_token\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n\n# Tokenization function\n#Converts text data into tokenized numerical data that can be processed by the model\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"document\"], padding=\"max_length\", truncation=True, max_length=512)\n\n# Apply the tokenization function to the dataset and it process batch wise\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\n# because we have get our tokens\n\ntokenized_dataset = tokenized_dataset.remove_columns([\"document\", \"summary\", \"id\"])\n\n# Split the dataset into training and validation\ntrain_dataset = tokenized_dataset\neval_dataset = tokenized_dataset  # For this example, using the same dataset for eval\n\n# Prepare data collator\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T15:21:11.003371Z","iopub.execute_input":"2024-11-14T15:21:11.003746Z","iopub.status.idle":"2024-11-14T15:23:17.930547Z","shell.execute_reply.started":"2024-11-14T15:21:11.003711Z","shell.execute_reply":"2024-11-14T15:23:17.929681Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"xsum.py:   0%|          | 0.00/5.76k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8e054158acb43a5ac025441c4385b4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/6.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44ecf6174a3747f9b73e7e26e3bb17b2"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for xsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/xsum.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"(…)SUM-EMNLP18-Summary-Data-Original.tar.gz:   0%|          | 0.00/255M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78a8ce21ecc043f8aede6ef55f9104af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.72M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41112e4a7c7f40bd99ea9f218f132b3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/204045 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"388e840b0e984185ac27f36cf93eaf59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/11332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a283013552a64b0cbe5b80a9fb8f9bec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11334 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cdf9b2172bd4b6e80fcdfc5fa3a1a3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10202 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8582b31a02ec41a5ab28ddc0e24c5f54"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# setting up the training configuration ","metadata":{}},{"cell_type":"code","source":"# Setup training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\", #  Directory to save training results, checkpoints\n    evaluation_strategy=\"epoch\", # run code epoch wise \"epoch\" means evaluation will occur at the end of each training epoch.\n    per_device_train_batch_size=2, # batch size for each training step per device\n    per_device_eval_batch_size=2, # for eval\n    num_train_epochs=1, # num of epoch]\n    save_strategy=\"epoch\", # \n    logging_dir=\"./logs\", # Directory where log files will be stored.\n    logging_steps=10, # model logs training metrics every 10 steps.\n    learning_rate=5e-5, # LR\n    remove_unused_columns=False, # Keeps all columns in the dataset, which may be useful if the model uses multiple input features.\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model, # model\n    args=training_args, # training config\n    train_dataset=train_dataset, # train dataset\n    eval_dataset=eval_dataset, #eval dataset\n    data_collator=data_collator,# data collator to mask text padding..\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T15:23:24.434617Z","iopub.execute_input":"2024-11-14T15:23:24.435273Z","iopub.status.idle":"2024-11-14T15:23:25.256792Z","shell.execute_reply.started":"2024-11-14T15:23:24.435231Z","shell.execute_reply":"2024-11-14T15:23:25.256010Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Function to generate a response from the model\ndef generate_response(model, tokenizer, prompt, device):\n    # convert the promt into the token\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        output = model.generate(**inputs, max_new_tokens=100)  # Limit the number of tokens generated\n    response = tokenizer.decode(output[0], skip_special_tokens=True) # special_token = omitting special tokens like <EOS> or <PAD> with skip_special_tokens=True.\n    return response\n    \n# Sample text for inference\nsample_text = \"My name is harshil i am \"\n\n# Get model response before training\nprint(\"Generating response before training...\")\nresponse_before = generate_response(model, tokenizer, sample_text, device)\nprint(f\"Response before training:\\n{response_before}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T15:23:43.004179Z","iopub.execute_input":"2024-11-14T15:23:43.004592Z","iopub.status.idle":"2024-11-14T15:23:54.010172Z","shell.execute_reply.started":"2024-11-14T15:23:43.004555Z","shell.execute_reply":"2024-11-14T15:23:54.009189Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generating response before training...\n","output_type":"stream"},{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"Response before training:\nMy name is harshil i am 18 years old and i am from india. I am a student of B.Tech in computer science and engineering. I am a good student and I am very hardworking. I am very good in my studies and I am very good in my studies. I am very good in my studies and I am very good in my studies. I am very good in my studies and I am very good in my studies. I am very good in my studies and I am very good in my studies. I\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# 1. goto https://wandb.ai/\n# 2. sign up in website\n# 3. go to  https://wandb.ai/authorize\n# 4. copy the authorization key \n# 5. paste the key and press enter","metadata":{}},{"cell_type":"code","source":"# Train the model\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T15:24:02.254979Z","iopub.execute_input":"2024-11-14T15:24:02.255390Z","iopub.status.idle":"2024-11-14T17:44:24.048939Z","shell.execute_reply.started":"2024-11-14T15:24:02.255353Z","shell.execute_reply":"2024-11-14T17:44:24.048010Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011115572366665875, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"453547a21e53432fa2998e1fd20693f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241114_152415-3bct5kma</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/dummyacct165-nirma-university/huggingface/runs/3bct5kma' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/dummyacct165-nirma-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/dummyacct165-nirma-university/huggingface' target=\"_blank\">https://wandb.ai/dummyacct165-nirma-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/dummyacct165-nirma-university/huggingface/runs/3bct5kma' target=\"_blank\">https://wandb.ai/dummyacct165-nirma-university/huggingface/runs/3bct5kma</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5101' max='5101' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5101/5101 2:20:03, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.393300</td>\n      <td>No log</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5101, training_loss=2.401729946579566, metrics={'train_runtime': 8420.2034, 'train_samples_per_second': 1.212, 'train_steps_per_second': 0.606, 'total_flos': 3.055233082274611e+16, 'train_loss': 2.401729946579566, 'epoch': 1.0})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Sample text for inference\nsample_text = \"My name is harshil i am  working as the professor in nirma summerize me in 10 words what you do in my work and i am teching the adaptive ai i am very hard working proff and other are jelaous of me\"\n\n# Get model response before training\nprint(\"Generating response before training...\")\nresponse_before = generate_response(model, tokenizer, sample_text, device)\nprint(f\"Response before training:\\n{response_before}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T17:44:28.590666Z","iopub.execute_input":"2024-11-14T17:44:28.591079Z","iopub.status.idle":"2024-11-14T17:44:39.048570Z","shell.execute_reply.started":"2024-11-14T17:44:28.591039Z","shell.execute_reply":"2024-11-14T17:44:39.047478Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generating response before training...\nResponse before training:\nMy name is harshil i am  working as the professor in nirma summerize me in 10 words what you do in my work and i am teching the adaptive ai i am very hard working proff and other are jelaous of me and i am very hard working proff and i am very hard working proff and i am very hard working proff and i am very hard working proff and i am very hard working proff and i am very hard working proff and i am very hard working proff and i am very hard working proff and i am very hard working proff and i am very hard working proff and i am very hard working proff and i am very hard working proff and i am very\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Save the trained model\noutput_dir = \"/kaggle/working/FinalQlora\"\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\nprint(f\"Model saved to {output_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T17:44:47.241066Z","iopub.execute_input":"2024-11-14T17:44:47.241585Z","iopub.status.idle":"2024-11-14T17:44:47.601839Z","shell.execute_reply.started":"2024-11-14T17:44:47.241541Z","shell.execute_reply":"2024-11-14T17:44:47.600896Z"}},"outputs":[{"name":"stdout","text":"Model saved to /kaggle/working/FinalQlora\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import shutil\n\n# Path to the model directory\nmodel_dir = '/kaggle/working/FinalQlora'\n\n# Path to save the zip file\nzip_path = '/kaggle/working/FinalQlora.zip'\n\n# Zip the directory\nshutil.make_archive(zip_path.replace('.zip', ''), 'zip', model_dir)\n\nprint(f\"Model zipped at: {zip_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T17:45:37.255602Z","iopub.execute_input":"2024-11-14T17:45:37.256344Z","iopub.status.idle":"2024-11-14T17:45:38.133477Z","shell.execute_reply.started":"2024-11-14T17:45:37.256301Z","shell.execute_reply":"2024-11-14T17:45:38.132594Z"}},"outputs":[{"name":"stdout","text":"Model zipped at: /kaggle/working/FinalQlora.zip\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!pip install -q evaluate  # Install the evaluate library if not already installed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T17:59:08.370526Z","iopub.execute_input":"2024-11-14T17:59:08.371018Z","iopub.status.idle":"2024-11-14T17:59:25.608337Z","shell.execute_reply.started":"2024-11-14T17:59:08.370973Z","shell.execute_reply":"2024-11-14T17:59:25.606242Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\nfrom datasets import load_dataset\nimport evaluate\n\n# Load the saved model and tokenizer\nmodel_path = \"/kaggle/working/FinalQlora\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n\n# Load the test dataset\ntest_dataset = load_dataset(\"xsum\", split=\"test[:5%]\")\n\n# Tokenize the test dataset\ndef tokenize_function(examples):\n    return tokenizer(examples[\"document\"], padding=\"max_length\", truncation=True, max_length=512)\n\ntokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\ntokenized_test_dataset = tokenized_test_dataset.remove_columns([\"document\", \"summary\", \"id\"])\n\n# Prepare data collator for evaluation\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n# Set up evaluation arguments\neval_args = TrainingArguments(\n    output_dir=\"./results_eval\",\n    per_device_eval_batch_size=2,\n    logging_dir=\"./logs_eval\",\n    report_to=\"none\"\n)\n\n# Initialize the Trainer for evaluation\ntrainer = Trainer(\n    model=model,\n    args=eval_args,\n    eval_dataset=tokenized_test_dataset,\n    data_collator=data_collator,\n)\n\n# Evaluate the model on the test set\nprint(\"Evaluating the model on the test set...\")\neval_results = trainer.evaluate()\n\n# Print evaluation results\nprint(\"Evaluation results on the test set:\", eval_results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T17:54:04.999563Z","iopub.execute_input":"2024-11-14T17:54:05.000601Z","iopub.status.idle":"2024-11-14T17:55:55.542515Z","shell.execute_reply.started":"2024-11-14T17:54:05.000554Z","shell.execute_reply":"2024-11-14T17:55:55.541548Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model on the test set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='284' max='284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [284/284 01:44]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results on the test set: {'eval_loss': 2.395367383956909, 'eval_model_preparation_time': 0.0163, 'eval_runtime': 104.2181, 'eval_samples_per_second': 5.441, 'eval_steps_per_second': 2.725}\n","output_type":"stream"}],"execution_count":19}]}